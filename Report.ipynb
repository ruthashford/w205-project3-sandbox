{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this report, we will demonstrate how to build a pipeline to enable a data science team to obtain valuable insights on game events.  \n",
    "\n",
    "The latest mobile game in a game development company has three events:  join a guild, purchase a sword, and purchase a horse.  To enable analytics for these events and the metadata of each of the events, a pipeline has been created to allow analysts to access the data.  \n",
    "\n",
    "To start a virtual container is created leveraging docker.\n",
    "\n",
    "Kafka is then leveraged for a singular topic called \"topics\" has been created to track the events of join a guild, purchase a sword, and purchase a sword.   A single topic is intentional to allow filtering.   \n",
    "\n",
    "A flask app is created to simulate a web app that allows events to be created and stored.  In the flask app, events can be triggered with the corresponding metadata and host information for analysis.  Values are saved in a dictionary in a single key to value to allow simplicity in reporting and scale.   Meaning, if additional values are added or existing values are replaced, the schema is easily adjustable.\n",
    "\n",
    "Python script filters for each event have been created and submits separated event tables to HDFS depending on the event type.  Error handling has also been added to ensure events are added.  This approach allows the availability to quickly adjust each event.  For new events, a separate python script can be created.  For example, if metadata is needed for swords the write_sword_events.py can be edited.  If a new event such as, purchase a spellbook is needed in the future, a wrtie_spellbook_events.py should be created.\n",
    "\n",
    "3 script files have been created to save results in tables written in Hadoop.\n",
    "\n",
    "* write_sword_events.py  \n",
    "* write_horse_events.py  \n",
    "* write_guild_events.py   \n",
    "\n",
    "Metadata to each event:\n",
    "\n",
    "* purchase_a_sword: accepts color and quantity parameters  \n",
    "* purchase_a_horse: accepts speed, size, and quantity parameters  \n",
    "* guild: accepts inputs to join or leave the guild  \n",
    "* default_response: this does not accept any parameters and is run when the user runs a blank input  \n",
    "\n",
    "Metadata for the users when an event is triggered outside of events and user agent:\n",
    "\n",
    "* timestamp:  information allowing analysis on when the event occurs.   Use this value for understanding state changes over time.\n",
    "* Host:  information for allowing analysis on the User Id.\n",
    "\n",
    "After each script is ran, the events are successfully landed into HDFS.\n",
    "\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/sword_purchases\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/horse_purchases\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/guild_actions\n",
    "\n",
    "External tables are then created to save data into the Hive metastore.\n",
    "\n",
    "sword_table\n",
    "horse_table\n",
    "guild_table\n",
    "\n",
    "Data is then queried through a tool.    In this example, presto is used for queries. \n",
    "\n",
    "Data is then saved in parquet for querying to answer the following business questions:\n",
    "\n",
    "* Guild Stats - How many people are in the guild?   \n",
    "* Guild Stats - How many people joined the guild in the past year?   \n",
    "* Horse Stats - How many horses have been purchased by size?   \n",
    "* Sword Stats - How many swords have been purchased by color?   \n",
    "* Sword Stats - How many swords have been purchased in the past year?  \n",
    "\n",
    "Tables are set up with a positive or negative quantity to determine the state (total value) of swords and horses and state of being in a guild.    To query the total, sum the quantity of swords and horses.   To determine the state of a guild, a value of +1 indicates in a guild and a -1 indicates not in a guild.\n",
    "\n",
    "**Creating Testing Data:**\n",
    "\n",
    "For unit testing, apache bench was leveraged to send batch records to create mock data into parquet and enable testing of reporting.    Scenarios of testing data was coded into apache bench, this approach ensures if 10 values were created we can expect the 10 values in the reporting/analysis.  A streaming script has been created called generate_data.sh file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Pipeline\n",
    "Firstly, we will showcase how we built the pipeline. We have chosen to use Kafka, Hadoop and Spark to transport, transform and store the game events. We have defined the configuration for each of these in the **docker-compose.yml** file, below is a description of each part of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zookeeper & Kafka\n",
    "Zookeeper allows us to easily access and manage the Kafka instance. Kafka allows us to easily create a pipeline that we can publish game events to, which can then be consumed by Spark.  \n",
    "\n",
    "We have opened ports in Zookeeper which are referenced in the Kafka configuration. This allows us to create topics in Kafka via Zookeeper. We have also exposed ports in Kafka that allow us to publish and consume messages from the topics.\n",
    "\n",
    "```bash\n",
    "zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop\n",
    "We have added Cloudera configuration to be able to use Hadoop with additional ports exposed to be able to connect to Hive.\n",
    "\n",
    "```bash\n",
    "  cloudera:\n",
    "    image: midsw205/hadoop:0.0.2\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"8888\" # hue\n",
    "      - \"9083\" # hive thrift\n",
    "      - \"10000\" # hive jdbc\n",
    "      - \"50070\" # nn http\n",
    "    ports:\n",
    "      - \"8888:8888\"      \n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "We have set up Spark using the MIDS W205 Spark Python base image. We're specifying the dependency of this service on Cloudera and the Hadoop name node that Spark will use when writing to HDFS.    \n",
    "In addition to this, we have exposed ports which allow us to connect to Spark from a notebook.\n",
    "\n",
    "```bash\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.6\n",
    "    stdin_open: true\n",
    "    tty: true   \n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "      - \"7000\" #jupyter notebook      \n",
    "    ports:\n",
    "      - \"7000:7000\" # map instance:service port   \n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presto\n",
    "We have added Presto configution as an alternative tool for data scientists to query the data.\n",
    "\n",
    "```bash\n",
    "  presto:\n",
    "    image: midsw205/presto:0.0.1\n",
    "    hostname: presto\n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    expose:\n",
    "      - \"8080\"\n",
    "    environment:\n",
    "      HIVE_THRIFTSERVER: cloudera:9083\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIDS Base Image\n",
    "This is the base image that we will use in the container, it allows us to run bash commands, as well as using kafkacat to publish messages to Kafka. We're specifying the w205 volume so that we have access to files.  \n",
    "\n",
    "```bash\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    expose:\n",
    "      - \"5000\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Up the Pipeline\n",
    "To do this we run the following command:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Then we create a events Kafka topic which we will publish all the game events to:\n",
    "```bash\n",
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting With the Pipeline\n",
    "\n",
    "### Flask App\n",
    "\n",
    "Our Flask app is built in the game_api.py file. The app is designed to connect with Kafka, so that any events generated by a user are captured in our Kafka topic called \"events\". The app consists of four different functions: \n",
    "\n",
    "1. `purchase_a_sword`: accepts color and quantity parameters \n",
    "2. `purchase_a_horse`: accepts speed, size, and quantity parameters\n",
    "3. `guild`: accepts inputs to join or leave the guild\n",
    "4. `default_response`: this does not accept any parameters and is run when the user runs a blank input\n",
    "\n",
    "Each function is structured as a post API request. As an example, see the code below for the purchase_a_horse function. The function accepts parameters in the API call and are captured in a dictionary. Next, we wrote error handling code so the function only accepts valid user inputs. Finally, the dictionary is written to our Kafka topic, \"events\", which captures the user inputs from any of our four functions.\n",
    "\n",
    "```python\n",
    "@app.route(\"/purchase_a_horse/<speed>/<size>/<quantity>\", methods=[\"POST\"])\n",
    "def purchase_a_horse(speed, size, quantity):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - speed\n",
    "    - size (small, medium, or large)\n",
    "    - quantity\n",
    "    \"\"\"\n",
    "    \n",
    "    # collect user inputs\n",
    "    purchase_horse_event = {\n",
    "        \"event_type\": \"purchase_horse\", \n",
    "        \"speed\": speed, \n",
    "        \"size\": size, \n",
    "        \"quantity\": quantity}\n",
    "    \n",
    "    # error handling\n",
    "    if purchase_horse_event['size'].lower() not in ['small', 'medium', 'large']:\n",
    "        raise Exception(\"Please enter either 'small', 'medium' or 'large' for horse size\")\n",
    "        \n",
    "    elif float(purchase_horse_event['speed']) < 0:\n",
    "        raise Exception(\"Please enter a non-negative value for speed\")\n",
    "        \n",
    "    elif float(purchase_horse_event['quantity']) < 0:\n",
    "        raise Exception(\"Please enter a non-negative value for quantity\")\n",
    "        \n",
    "    else:\n",
    "        # clean inputs to collect only lower case values for consistency\n",
    "        purchase_horse_event['size'] = purchase_horse_event['size'].lower()\n",
    "        \n",
    "        # log event to kafka\n",
    "        log_to_kafka(\"events\", purchase_horse_event)\n",
    "        return \"Horse Purchased!\\n\"\n",
    "```\n",
    "\n",
    "\n",
    "### Extracting Events\n",
    "\n",
    "After the users have used the app and generated events stored in the Kafka topic, we utilized Pyspark to extract the events and land them in HDFS. We wrote three separate Python scripts to individually extract data for each event type (sword, horse, and guild). Each script consists of four main steps:   \n",
    "1. Creating a Hive table \n",
    "2. Read the events from the Kafka topic as a stream\n",
    "3. Filter them to extract events of a specific type\n",
    "4. Write them to HDFS tables as a stream in 10 second batch intervals.   \n",
    "\n",
    "We have three separate HDFS tables with their own unique schema that are specific to the event type.\n",
    "\n",
    "As an example, the code below shows how we use Spark streaming to filter the horse events and write them to HDFS in parquet format. \n",
    "\n",
    "```python\n",
    "horse_purchases = raw_events \\\n",
    "    .filter(is_horse_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_horse_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "sink = horse_purchases \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints_for_horse_purchases\") \\\n",
    "    .option(\"path\", \"/tmp/horse_purchases\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()  \n",
    "        \n",
    "sink.awaitTermination()\n",
    "```\n",
    "\n",
    "### Starting the Stream\n",
    "Running the following three commands will set up a stream for each of the event types. As new events come in, they are automatically read from Kafka, written to HDFS and available in Hive to be queried using Presto.\n",
    "\n",
    "```bash\n",
    "docker-compose exec spark spark-submit /w205/w205-project3-sandbox/write_sword_events.py\n",
    "docker-compose exec spark spark-submit /w205/w205-project3-sandbox/write_horse_events.py\n",
    "docker-compose exec spark spark-submit /w205/w205-project3-sandbox/write_guild_events.py\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data\n",
    "Now that the Spark streaming scripts are waiting for data to be generated, we can generate events using Apache bench.\n",
    "\n",
    "Use the [generate_data.sh file](https://github.com/ruthashford/w205-project3-sandbox/blob/main/generate_data.sh) to send standardized events in bulk via Apache Bench: \n",
    "\n",
    "```bash\n",
    "sh generate_data.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "The resulting tables written in Hadoop follow the structures in the following ERD diagram:\n",
    "\n",
    "<img src=\"W205 Project 3 ERD.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Events\n",
    "\n",
    "Now that we have event data generated and it has successfully gone through the pipeline, we can use both Presto and PySpark to query it to help answer business questions.  \n",
    "\n",
    "### Presto\n",
    "Open Presto using the following command:\n",
    "```bash\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "```\n",
    "\n",
    "Check to see what tables are available in presto: \n",
    "```bash\n",
    "presto:default> show tables;\n",
    "```\n",
    "\n",
    "Output:\n",
    "```bash\n",
    "     Table     \n",
    "---------------\n",
    " guild_actions   \n",
    " horse_purchases \n",
    " sword_purchases\n",
    "(3 rows)\n",
    "```\n",
    "\n",
    "#### Example Queries \n",
    "Showing some useful fields in our swords table\n",
    "```sql\n",
    "SELECT color, quantity FROM sword_purchases LIMIT 5;\n",
    "```\n",
    "\n",
    "Output\n",
    "```bash\n",
    " color | quantity \n",
    "-------+----------\n",
    " red   | 2        \n",
    " red   | 2        \n",
    " red   | 2        \n",
    " red   | 2        \n",
    " red   | 2        \n",
    "(5 rows)\n",
    "```\n",
    "\n",
    "How many horses have been purchased by size? \n",
    "```sql\n",
    "SELECT\n",
    "  size AS horse_size,\n",
    "  SUM(CAST(quantity as INTEGER)) AS num_horses\n",
    "FROM horse_purchases\n",
    "GROUP BY 1;\n",
    "```\n",
    "\n",
    "Output:\n",
    "```bash\n",
    " horse_size | num_horses \n",
    "------------+------------\n",
    " small      |        800 \n",
    " medium     |       1100 \n",
    " large      |        600 \n",
    "```\n",
    "\n",
    "### PySpark\n",
    "As an alternative to Presto, we can also use PySpark to run queries directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sword_purchases = spark.read.parquet('/tmp/sword_purchases')\n",
    "sword_purchases.registerTempTable('sword_purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|year|num_swords_purchaed|\n",
      "+----+-------------------+\n",
      "|2021|               2160|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many sword were purchased in the last year?\n",
    "spark.sql('SELECT \\\n",
    "    year, \\\n",
    "    sum(num_swords_purchaed) as num_swords_purchaed \\\n",
    "    FROM( \\\n",
    "        SELECT \\\n",
    "            YEAR(timestamp) as year, \\\n",
    "            CAST(quantity AS INTEGER) AS num_swords_purchaed \\\n",
    "        FROM sword_purchases \\\n",
    "    ) \\\n",
    "GROUP BY year').show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
