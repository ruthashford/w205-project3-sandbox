{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**TO-DO:** Keeping for reference before submission. Insert more details here on what can be done in the pipeline, and the business questions we will be answering.\n",
    "\n",
    "In this report, we will demonstate how to build a pipeline to enable a data science team to obtain valuable insights on game events.  \n",
    "\n",
    "The latest mobile game in a game development company has three events:  join a guild, purchase a sword, and purchase a horse.  To enable analytics for these events and the metadata of each of the events, a pipeline has been created to allow analysts to access the data.  \n",
    "\n",
    "In kafka a singular topic called \"topics\" has been created to track the events of join a guild, purchase a sword, and purchase a sword.   A single topic is intentional to allow filtering.   \n",
    "\n",
    "Python script filters for each event have been created and submits separated event tables to hdfs depending on the event type.  Error handling has also been added to ensure events are added.  This approach allows the availability to quickly adjust each event.  For new events, a seperate python script can be created.  For example, if meta data is needed for swords the write_sword_events.py can be edited.  If a new event such as, purchase a spell book is needed in the future, a wrtie_spellbook_events.py should be created.\n",
    "\n",
    "In the flask app, events can be triggered with the corresponding meta data and host information for anlaysis.  Values are saved in a dictionary in a single key to value to allow simplicity in reporting and scale.   Meaning, if additional values are added or exisitng values are replaced, the schema is easily adjustable.\n",
    "\n",
    "Meta data to each event:\n",
    "\n",
    "* purchase_a_sword: accepts color and quantity parameters  \n",
    "* purchase_a_horse: accepts speed, size, and quantity parameters  \n",
    "* guild: accepts inputs to join or leave the guild  \n",
    "* default_response: this does not accept any parameters and is run when the user runs a blank input  \n",
    "\n",
    "Meta data for the users when an event is triggered outside of events and user agent:\n",
    "\n",
    "* timestamp:  information allowing analysis on when the event occurs.   Use this value for understanding state changes over time.\n",
    "* Host:  information for allowing analysis on the User Id.\n",
    "\n",
    "Data is then saved in parquet for querying to answer the following business questions:\n",
    "\n",
    "* Guild Stats - How many people are in the guild?   \n",
    "* Guild Stats - How many people joined the guild in the past year?   \n",
    "* Horse Stats - How many horses have been purchased by size?   \n",
    "* Sword Stats - How many swords have been purchased by color?   \n",
    "* Sword Stats - How many swords have been purchased in the past year?  \n",
    "\n",
    "Tables are setup with a positive or negative quantity to determine the state (total value) of swords and horses and state of being in a guild.    To query the total, sum the quantity of swords and horses.   To determine the state of a guild, a value of +1 indicates in a guild and a -1 indicates not in a guild.\n",
    "\n",
    "For unit testing, apache bench was leveraged to send batch records to create mock data into parquet and enable testing of reporting.    Scenarios of testing data was coded into apache bench this approach ensures if 10 values were created we can expect the 10 values in the reporting/analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipeline\n",
    "Firstly, we will showcase how we built the pipeline. We have chosen to use Kafka, Hadoop and Spark to transport, transform and store the game events. We have defined the configuration for each of these in the **docker-compose.yml** file, below is a description of each part of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zookeeper & Kafka\n",
    "Zookeeper allows us to easily access and manage the Kafka instance. Kafka allows us to easily create a pipeline that we can publish game events to, which can then be consumed by Spark.  \n",
    "\n",
    "We have opened ports in Zookeeper we have have also reference in the Kafka configuration, this allows us to create topics in Kafka via Zookeeper. We have also exposed ports in Kafka that allow us to publish and consume messages from the topics.\n",
    "\n",
    "```bash\n",
    "zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop\n",
    "We have added Cloudera configuration to be able to use Hadoop.\n",
    "\n",
    "```bash\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "We have set up Spark using the MIDS W205 Spark Python base image. We're specifying the dependencyof this service on Cloudera and the Hadoop name node that Spark will use when writing to HDFS.    \n",
    "In addition to this, we have exposed additional ports which allow us to connect to Spark from a notebook.\n",
    "\n",
    "```bash\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true   \n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "      - \"7000\" #jupyter notebook      \n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "      - \"7000:7000\" # map instance:service port   \n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIDS Base Image\n",
    "This is the base image that we will use in the container, it allows us to run bash commands, as well as using kafkacat to publish messages to Kafka. We're specifying the w205 volume so that we have access to files.  \n",
    "\n",
    "```bash\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    expose:\n",
    "      - \"5000\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring up the pipeline\n",
    "To do this we run the following command:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the pipeline\n",
    "\n",
    "### Flask app\n",
    "\n",
    "Our Flask app is built in the game_api.py file. The app is designed to connect with Kafka, so that any events generated by a user are captured in our Kafla topic called \"events\". The app consists of four different functions: \n",
    "\n",
    "1. `purchase_a_sword`: accepts color and quantity parameters \n",
    "2. `purchase_a_horse`: accepts speed, size, and quantity parameters\n",
    "3. `guild`: accepts inputs to join or leave the guild\n",
    "4. `default_response`: this does not accept any parameters and is run when the user runs a blank input\n",
    "\n",
    "Each function is structured as a post API request. As an example, see the code below for the purchase_a_horse function. The function accepts parameters in the API call and are captured in a dictionary. Next, we wrote error handling code so the function only accepts valid user inputs. Finally, the dictionary is written to our Kafka topic, \"events\", which captures the user inputs from any of our four functions.\n",
    "\n",
    "```python\n",
    "@app.route(\"/purchase_a_horse/<speed>/<size>/<quantity>\", methods=[\"POST\"])\n",
    "def purchase_a_horse(speed, size, quantity):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - speed\n",
    "    - size (small, medium, or large)\n",
    "    - quantity\n",
    "    \"\"\"\n",
    "    \n",
    "    # collect user inputs\n",
    "    purchase_horse_event = {\n",
    "        \"event_type\": \"purchase_horse\", \n",
    "        \"speed\": speed, \n",
    "        \"size\": size, \n",
    "        \"quantity\": quantity}\n",
    "    \n",
    "    # error handling\n",
    "    if purchase_horse_event['size'].lower() not in ['small', 'medium', 'large']:\n",
    "        raise Exception(\"Please enter either 'small', 'medium' or 'large' for horse size\")\n",
    "        \n",
    "    elif float(purchase_horse_event['speed']) < 0:\n",
    "        raise Exception(\"Please enter a non-negative value for speed\")\n",
    "        \n",
    "    elif float(purchase_horse_event['quantity']) < 0:\n",
    "        raise Exception(\"Please enter a non-negative value for quantity\")\n",
    "        \n",
    "    else:\n",
    "        # clean inputs to collect only lower case values for consistency\n",
    "        purchase_horse_event['size'] = purchase_horse_event['size'].lower()\n",
    "        \n",
    "        # log event to kafka\n",
    "        log_to_kafka(\"events\", purchase_horse_event)\n",
    "        return \"Horse Purchased!\\n\"\n",
    "```\n",
    "\n",
    "\n",
    "### Extracting events\n",
    "\n",
    "After the users have used the app and generated events stored in the Kafka topic, we utilized Pyspark to extract the events and land them in HDFS. We wrote three separate Python scripts to individually extract data for each event type (sword, horse, and guild). Each script consists of three main steps: (1) read the events from the Kafka topic, (2) filter them to extract events of a specific type, and (3) write them to HDFS tables. We have three separate HDFS tables with their own unique schema that are specific to the event type.\n",
    "\n",
    "As an example, the code below shows how we filter the horse events and write them to HDFS in parquet format. \n",
    "\n",
    "```python\n",
    "horse_purchases = raw_events \\\n",
    "    .filter(is_horse_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_horse_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "horse_purchases.show()\n",
    "horse_purchases.write.mode(\"overwrite\")\\\n",
    "    .parquet(\"/tmp/horse_purchases\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the events\n",
    "**TO-DO:** business analysis goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sword_purchases = spark.read.parquet('/tmp/sword_purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-----------------+---------------+--------------+-----+--------+\n",
      "|           raw_event|           timestamp|Accept|             Host|     User-Agent|    event_type|color|quantity|\n",
      "+--------------------+--------------------+------+-----------------+---------------+--------------+-----+--------+\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "+--------------------+--------------------+------+-----------------+---------------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sword_purchases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----+\n",
      "|             Host|color|total|\n",
      "+-----------------+-----+-----+\n",
      "|user1.comcast.com|  red| 10.0|\n",
      "+-----------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sword_purchases.registerTempTable('sword_purchases')\n",
    "total_swords = spark.sql(\"select Host, color, sum(quantity) total from sword_purchases group by Host, color\")\n",
    "total_swords.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
