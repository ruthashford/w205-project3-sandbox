{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this report, we will demonstate how to build a pipeline to enable a data science team to obtain valuable insights on game events.  \n",
    "**TO-DO:** Insert more details here on what can be done in the pipeline, and the business questions we will be answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the pipeline\n",
    "Firstly, we will showcase how we built the pipeline. We have chosen to use Kafka, Hadoop and Spark to transport, transform and store the game events. We have defined the configuration for each of these in the **docker-compose.yml** file, below is a description of each part of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zookeeper & Kafka\n",
    "Zookeeper allows us to easily access and manage the Kafka instance. Kafka allows us to easily create a pipeline that we can publish game events to, which can then be consumed by Spark.  \n",
    "\n",
    "We have opened ports in Zookeeper we have have also reference in the Kafka configuration, this allows us to create topics in Kafka via Zookeeper. We have also exposed ports in Kafka that allow us to publish and consume messages from the topics.\n",
    "\n",
    "```bash\n",
    "zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop\n",
    "We have added Cloudera configuration to be able to use Hadoop.\n",
    "\n",
    "```bash\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "      - \"8888\" # hue\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "We have set up Spark using the MIDS W205 Spark Python base image. We're specifying the dependencyof this service on Cloudera and the Hadoop name node that Spark will use when writing to HDFS.    \n",
    "In addition to this, we have exposed additional ports which allow us to connect to Spark from a notebook.\n",
    "\n",
    "```bash\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true   \n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "      - \"7000\" #jupyter notebook      \n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "      - \"7000:7000\" # map instance:service port   \n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIDS Base Image\n",
    "This is the base image that we will use in the container, it allows us to run bash commands, as well as using kafkacat to publish messages to Kafka. We're specifying the w205 volume so that we have access to files.  \n",
    "\n",
    "```bash\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    expose:\n",
    "      - \"5000\"\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "    volumes:\n",
    "      - \"~/w205:/w205\"\n",
    "    extra_hosts:\n",
    "      - \"moby:127.0.0.1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring up the pipeline\n",
    "To do this we run the following command:\n",
    "\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with the pipeline\n",
    "\n",
    "### Flask app\n",
    "\n",
    "Our Flask app is built in the game_api.py file. The app is designed to connect with Kafka, so that any events generated by a user are captured in our Kafla topic called \"events\". The app consists of four different functions: \n",
    "\n",
    "1. `purchase_a_sword`: accepts color and quantity parameters \n",
    "2. `purchase_a_horse`: accepts speed, size, and quantity parameters\n",
    "3. `guild`: accepts inputs to join or leave the guild\n",
    "4. `default_response`: this does not accept any parameters and is run when the user runs a blank input\n",
    "\n",
    "Each function is structured as a post API request. As an example, see the code below for the purchase_a_horse function. The function accepts parameters in the API call and are captured in a dictionary. Next, we wrote error handling code so the function only accepts valid user inputs. Finally, the dictionary is written to our Kafka topic, \"events\", which captures the user inputs from any of our four functions.\n",
    "\n",
    "```python\n",
    "@app.route(\"/purchase_a_horse/<speed>/<size>/<quantity>\", methods=[\"POST\"])\n",
    "def purchase_a_horse(speed, size, quantity):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - speed\n",
    "    - size (small, medium, or large)\n",
    "    - quantity\n",
    "    \"\"\"\n",
    "    \n",
    "    # collect user inputs\n",
    "    purchase_horse_event = {\n",
    "        \"event_type\": \"purchase_horse\", \n",
    "        \"speed\": speed, \n",
    "        \"size\": size, \n",
    "        \"quantity\": quantity}\n",
    "    \n",
    "    # error handling\n",
    "    if purchase_horse_event['size'].lower() not in ['small', 'medium', 'large']:\n",
    "        raise Exception(\"Please enter either 'small', 'medium' or 'large' for horse size\")\n",
    "        \n",
    "    elif float(purchase_horse_event['speed']) < 0:\n",
    "        raise Exception(\"Please enter a non-negative value for speed\")\n",
    "        \n",
    "    elif float(purchase_horse_event['quantity']) < 0:\n",
    "        raise Exception(\"Please enter a non-negative value for quantity\")\n",
    "        \n",
    "    else:\n",
    "        # clean inputs to collect only lower case values for consistency\n",
    "        purchase_horse_event['size'] = purchase_horse_event['size'].lower()\n",
    "        \n",
    "        # log event to kafka\n",
    "        log_to_kafka(\"events\", purchase_horse_event)\n",
    "        return \"Horse Purchased!\\n\"\n",
    "```\n",
    "\n",
    "\n",
    "### Extracting events\n",
    "\n",
    "After the users have used the app and generated events stored in the Kafka topic, we utilized Pyspark to extract the events and land them in HDFS. We wrote three separate Python scripts to individually extract data for each event type (sword, horse, and guild). Each script consists of three main steps: (1) read the events from the Kafka topic, (2) filter them to extract events of a specific type, and (3) write them to HDFS tables. We have three separate HDFS tables with their own unique schema that are specific to the event type.\n",
    "\n",
    "As an example, the code below shows how we filter the horse events and write them to HDFS in parquet format. \n",
    "\n",
    "```python\n",
    "horse_purchases = raw_events \\\n",
    "    .filter(is_horse_purchase(raw_events.value.cast('string'))) \\\n",
    "    .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "            raw_events.timestamp.cast('string'),\n",
    "            from_json(raw_events.value.cast('string'),\n",
    "                      purchase_horse_event_schema()).alias('json')) \\\n",
    "    .select('raw_event', 'timestamp', 'json.*')\n",
    "\n",
    "horse_purchases.show()\n",
    "horse_purchases.write.mode(\"overwrite\")\\\n",
    "    .parquet(\"/tmp/horse_purchases\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the events\n",
    "**TO-DO:** business analysis goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sword_purchases = spark.read.parquet('/tmp/sword_purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+-----------------+---------------+--------------+-----+--------+\n",
      "|           raw_event|           timestamp|Accept|             Host|     User-Agent|    event_type|color|quantity|\n",
      "+--------------------+--------------------+------+-----------------+---------------+--------------+-----+--------+\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "|{\"event_type\": \"p...|2021-07-20 03:47:...|   */*|user1.comcast.com|ApacheBench/2.3|purchase_sword|  red|       2|\n",
      "+--------------------+--------------------+------+-----------------+---------------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sword_purchases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-----+\n",
      "|             Host|color|total|\n",
      "+-----------------+-----+-----+\n",
      "|user1.comcast.com|  red| 10.0|\n",
      "+-----------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sword_purchases.registerTempTable('sword_purchases')\n",
    "total_swords = spark.sql(\"select Host, color, sum(quantity) total from sword_purchases group by Host, color\")\n",
    "total_swords.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
